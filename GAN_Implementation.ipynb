{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJWSreGDMbjg",
        "outputId": "7409646e-2d88-4ae9-b072-75a033ed95b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 56.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.67MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.3MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.28MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/100] [Batch 0/938] [D loss: 0.7061] [G loss: 0.6776]\n",
            "[Epoch 0/100] [Batch 200/938] [D loss: 0.0432] [G loss: 2.5940]\n",
            "[Epoch 0/100] [Batch 400/938] [D loss: 0.3094] [G loss: 2.1380]\n",
            "[Epoch 0/100] [Batch 600/938] [D loss: 0.1768] [G loss: 2.8983]\n",
            "[Epoch 0/100] [Batch 800/938] [D loss: 0.2819] [G loss: 2.8275]\n",
            "[Epoch 1/100] [Batch 0/938] [D loss: 0.1696] [G loss: 3.7236]\n",
            "[Epoch 1/100] [Batch 200/938] [D loss: 0.1874] [G loss: 4.4389]\n",
            "[Epoch 1/100] [Batch 400/938] [D loss: 0.1335] [G loss: 3.9873]\n",
            "[Epoch 1/100] [Batch 600/938] [D loss: 0.1258] [G loss: 5.0432]\n",
            "[Epoch 1/100] [Batch 800/938] [D loss: 0.1508] [G loss: 4.0957]\n",
            "[Epoch 2/100] [Batch 0/938] [D loss: 0.0832] [G loss: 4.4976]\n",
            "[Epoch 2/100] [Batch 200/938] [D loss: 0.2101] [G loss: 4.3432]\n",
            "[Epoch 2/100] [Batch 400/938] [D loss: 0.0556] [G loss: 4.7394]\n",
            "[Epoch 2/100] [Batch 600/938] [D loss: 0.1160] [G loss: 4.2406]\n",
            "[Epoch 2/100] [Batch 800/938] [D loss: 0.0454] [G loss: 5.6513]\n",
            "[Epoch 3/100] [Batch 0/938] [D loss: 0.0944] [G loss: 8.3872]\n",
            "[Epoch 3/100] [Batch 200/938] [D loss: 0.1634] [G loss: 11.2948]\n",
            "[Epoch 3/100] [Batch 400/938] [D loss: 0.0714] [G loss: 9.5199]\n",
            "[Epoch 3/100] [Batch 600/938] [D loss: 0.0550] [G loss: 4.1499]\n",
            "[Epoch 3/100] [Batch 800/938] [D loss: 0.0177] [G loss: 5.9977]\n",
            "[Epoch 4/100] [Batch 0/938] [D loss: 0.1006] [G loss: 6.3474]\n",
            "[Epoch 4/100] [Batch 200/938] [D loss: 0.0253] [G loss: 7.3947]\n",
            "[Epoch 4/100] [Batch 400/938] [D loss: 0.0482] [G loss: 8.0695]\n",
            "[Epoch 4/100] [Batch 600/938] [D loss: 0.0958] [G loss: 7.5681]\n",
            "[Epoch 4/100] [Batch 800/938] [D loss: 0.0552] [G loss: 5.3963]\n",
            "[Epoch 5/100] [Batch 0/938] [D loss: 0.0639] [G loss: 4.9923]\n",
            "[Epoch 5/100] [Batch 200/938] [D loss: 0.0823] [G loss: 6.9601]\n",
            "[Epoch 5/100] [Batch 400/938] [D loss: 0.0357] [G loss: 8.2386]\n",
            "[Epoch 5/100] [Batch 600/938] [D loss: 0.0642] [G loss: 5.4319]\n",
            "[Epoch 5/100] [Batch 800/938] [D loss: 0.2599] [G loss: 7.1015]\n",
            "[Epoch 6/100] [Batch 0/938] [D loss: 0.2392] [G loss: 6.8169]\n",
            "[Epoch 6/100] [Batch 200/938] [D loss: 0.0725] [G loss: 5.6885]\n",
            "[Epoch 6/100] [Batch 400/938] [D loss: 0.1393] [G loss: 5.2636]\n",
            "[Epoch 6/100] [Batch 600/938] [D loss: 0.0806] [G loss: 8.6901]\n",
            "[Epoch 6/100] [Batch 800/938] [D loss: 0.0477] [G loss: 5.5585]\n",
            "[Epoch 7/100] [Batch 0/938] [D loss: 0.0544] [G loss: 6.5725]\n",
            "[Epoch 7/100] [Batch 200/938] [D loss: 0.1456] [G loss: 4.5758]\n",
            "[Epoch 7/100] [Batch 400/938] [D loss: 0.0876] [G loss: 6.2197]\n",
            "[Epoch 7/100] [Batch 600/938] [D loss: 0.0461] [G loss: 6.0442]\n",
            "[Epoch 7/100] [Batch 800/938] [D loss: 0.0560] [G loss: 5.8091]\n",
            "[Epoch 8/100] [Batch 0/938] [D loss: 0.0363] [G loss: 4.8210]\n",
            "[Epoch 8/100] [Batch 200/938] [D loss: 0.1438] [G loss: 6.2611]\n",
            "[Epoch 8/100] [Batch 400/938] [D loss: 0.2079] [G loss: 4.6460]\n",
            "[Epoch 8/100] [Batch 600/938] [D loss: 0.1473] [G loss: 3.6575]\n",
            "[Epoch 8/100] [Batch 800/938] [D loss: 0.1615] [G loss: 5.0157]\n",
            "[Epoch 9/100] [Batch 0/938] [D loss: 0.0656] [G loss: 4.8105]\n",
            "[Epoch 9/100] [Batch 200/938] [D loss: 0.1323] [G loss: 6.1752]\n",
            "[Epoch 9/100] [Batch 400/938] [D loss: 0.0968] [G loss: 4.7593]\n",
            "[Epoch 9/100] [Batch 600/938] [D loss: 0.1562] [G loss: 5.1505]\n",
            "[Epoch 9/100] [Batch 800/938] [D loss: 0.0924] [G loss: 4.9820]\n",
            "[Epoch 10/100] [Batch 0/938] [D loss: 0.1071] [G loss: 3.3714]\n",
            "[Epoch 10/100] [Batch 200/938] [D loss: 0.2144] [G loss: 5.0976]\n",
            "[Epoch 10/100] [Batch 400/938] [D loss: 0.0910] [G loss: 5.4272]\n",
            "[Epoch 10/100] [Batch 600/938] [D loss: 0.0525] [G loss: 4.6797]\n",
            "[Epoch 10/100] [Batch 800/938] [D loss: 0.1858] [G loss: 3.9428]\n",
            "[Epoch 11/100] [Batch 0/938] [D loss: 0.0619] [G loss: 4.2733]\n",
            "[Epoch 11/100] [Batch 200/938] [D loss: 0.2297] [G loss: 4.4193]\n",
            "[Epoch 11/100] [Batch 400/938] [D loss: 0.1747] [G loss: 4.6365]\n",
            "[Epoch 11/100] [Batch 600/938] [D loss: 0.1081] [G loss: 4.1095]\n",
            "[Epoch 11/100] [Batch 800/938] [D loss: 0.1039] [G loss: 8.2403]\n",
            "[Epoch 12/100] [Batch 0/938] [D loss: 0.1547] [G loss: 7.4869]\n",
            "[Epoch 12/100] [Batch 200/938] [D loss: 0.2127] [G loss: 4.2393]\n",
            "[Epoch 12/100] [Batch 400/938] [D loss: 0.0818] [G loss: 5.6475]\n",
            "[Epoch 12/100] [Batch 600/938] [D loss: 0.0626] [G loss: 5.0164]\n",
            "[Epoch 12/100] [Batch 800/938] [D loss: 0.1139] [G loss: 3.3887]\n",
            "[Epoch 13/100] [Batch 0/938] [D loss: 0.1015] [G loss: 5.4840]\n",
            "[Epoch 13/100] [Batch 200/938] [D loss: 0.1166] [G loss: 5.9702]\n",
            "[Epoch 13/100] [Batch 400/938] [D loss: 0.0531] [G loss: 7.3300]\n",
            "[Epoch 13/100] [Batch 600/938] [D loss: 0.0730] [G loss: 5.3560]\n",
            "[Epoch 13/100] [Batch 800/938] [D loss: 0.1068] [G loss: 7.5489]\n",
            "[Epoch 14/100] [Batch 0/938] [D loss: 0.1459] [G loss: 3.3627]\n",
            "[Epoch 14/100] [Batch 200/938] [D loss: 0.0479] [G loss: 5.5534]\n",
            "[Epoch 14/100] [Batch 400/938] [D loss: 0.2504] [G loss: 4.9553]\n",
            "[Epoch 14/100] [Batch 600/938] [D loss: 0.2731] [G loss: 4.6905]\n",
            "[Epoch 14/100] [Batch 800/938] [D loss: 0.1900] [G loss: 5.1577]\n",
            "[Epoch 15/100] [Batch 0/938] [D loss: 0.1081] [G loss: 3.7301]\n",
            "[Epoch 15/100] [Batch 200/938] [D loss: 0.1250] [G loss: 5.4921]\n",
            "[Epoch 15/100] [Batch 400/938] [D loss: 0.1224] [G loss: 4.3650]\n",
            "[Epoch 15/100] [Batch 600/938] [D loss: 0.1151] [G loss: 4.8896]\n",
            "[Epoch 15/100] [Batch 800/938] [D loss: 0.2300] [G loss: 3.7120]\n",
            "[Epoch 16/100] [Batch 0/938] [D loss: 0.1898] [G loss: 3.5523]\n",
            "[Epoch 16/100] [Batch 200/938] [D loss: 0.2281] [G loss: 4.1395]\n",
            "[Epoch 16/100] [Batch 400/938] [D loss: 0.1403] [G loss: 4.1744]\n",
            "[Epoch 16/100] [Batch 600/938] [D loss: 0.1763] [G loss: 4.7219]\n",
            "[Epoch 16/100] [Batch 800/938] [D loss: 0.0866] [G loss: 4.0127]\n",
            "[Epoch 17/100] [Batch 0/938] [D loss: 0.1713] [G loss: 4.6405]\n",
            "[Epoch 17/100] [Batch 200/938] [D loss: 0.1826] [G loss: 4.0303]\n",
            "[Epoch 17/100] [Batch 400/938] [D loss: 0.1929] [G loss: 4.5849]\n",
            "[Epoch 17/100] [Batch 600/938] [D loss: 0.2176] [G loss: 4.8852]\n",
            "[Epoch 17/100] [Batch 800/938] [D loss: 0.0912] [G loss: 3.8180]\n",
            "[Epoch 18/100] [Batch 0/938] [D loss: 0.1988] [G loss: 3.8177]\n",
            "[Epoch 18/100] [Batch 200/938] [D loss: 0.2326] [G loss: 4.5519]\n",
            "[Epoch 18/100] [Batch 400/938] [D loss: 0.1954] [G loss: 5.2492]\n",
            "[Epoch 18/100] [Batch 600/938] [D loss: 0.1733] [G loss: 4.3432]\n",
            "[Epoch 18/100] [Batch 800/938] [D loss: 0.2121] [G loss: 3.8269]\n",
            "[Epoch 19/100] [Batch 0/938] [D loss: 0.3229] [G loss: 4.8867]\n",
            "[Epoch 19/100] [Batch 200/938] [D loss: 0.3146] [G loss: 2.1830]\n",
            "[Epoch 19/100] [Batch 400/938] [D loss: 0.3265] [G loss: 4.5966]\n",
            "[Epoch 19/100] [Batch 600/938] [D loss: 0.2673] [G loss: 3.9523]\n",
            "[Epoch 19/100] [Batch 800/938] [D loss: 0.2512] [G loss: 3.7131]\n",
            "[Epoch 20/100] [Batch 0/938] [D loss: 0.2837] [G loss: 2.7732]\n",
            "[Epoch 20/100] [Batch 200/938] [D loss: 0.3729] [G loss: 2.3195]\n",
            "[Epoch 20/100] [Batch 400/938] [D loss: 0.1660] [G loss: 4.3739]\n",
            "[Epoch 20/100] [Batch 600/938] [D loss: 0.2001] [G loss: 2.6988]\n",
            "[Epoch 20/100] [Batch 800/938] [D loss: 0.3142] [G loss: 3.2084]\n",
            "[Epoch 21/100] [Batch 0/938] [D loss: 0.2475] [G loss: 3.0479]\n",
            "[Epoch 21/100] [Batch 200/938] [D loss: 0.1768] [G loss: 2.4898]\n",
            "[Epoch 21/100] [Batch 400/938] [D loss: 0.2109] [G loss: 3.4226]\n",
            "[Epoch 21/100] [Batch 600/938] [D loss: 0.2660] [G loss: 3.8890]\n",
            "[Epoch 21/100] [Batch 800/938] [D loss: 0.1171] [G loss: 5.0006]\n",
            "[Epoch 22/100] [Batch 0/938] [D loss: 0.1612] [G loss: 3.5841]\n",
            "[Epoch 22/100] [Batch 200/938] [D loss: 0.1773] [G loss: 2.8508]\n",
            "[Epoch 22/100] [Batch 400/938] [D loss: 0.2421] [G loss: 3.4685]\n",
            "[Epoch 22/100] [Batch 600/938] [D loss: 0.1479] [G loss: 4.2902]\n",
            "[Epoch 22/100] [Batch 800/938] [D loss: 0.1721] [G loss: 3.4454]\n",
            "[Epoch 23/100] [Batch 0/938] [D loss: 0.1456] [G loss: 2.8326]\n",
            "[Epoch 23/100] [Batch 200/938] [D loss: 0.2027] [G loss: 3.5557]\n",
            "[Epoch 23/100] [Batch 400/938] [D loss: 0.1758] [G loss: 3.1953]\n",
            "[Epoch 23/100] [Batch 600/938] [D loss: 0.2069] [G loss: 4.2859]\n",
            "[Epoch 23/100] [Batch 800/938] [D loss: 0.1532] [G loss: 4.7781]\n",
            "[Epoch 24/100] [Batch 0/938] [D loss: 0.1582] [G loss: 3.1477]\n",
            "[Epoch 24/100] [Batch 200/938] [D loss: 0.3351] [G loss: 2.3057]\n",
            "[Epoch 24/100] [Batch 400/938] [D loss: 0.1894] [G loss: 2.6984]\n",
            "[Epoch 24/100] [Batch 600/938] [D loss: 0.1348] [G loss: 3.3663]\n",
            "[Epoch 24/100] [Batch 800/938] [D loss: 0.2349] [G loss: 3.5448]\n",
            "[Epoch 25/100] [Batch 0/938] [D loss: 0.2486] [G loss: 5.0638]\n",
            "[Epoch 25/100] [Batch 200/938] [D loss: 0.2265] [G loss: 2.5377]\n",
            "[Epoch 25/100] [Batch 400/938] [D loss: 0.2029] [G loss: 4.1964]\n",
            "[Epoch 25/100] [Batch 600/938] [D loss: 0.1485] [G loss: 3.7977]\n",
            "[Epoch 25/100] [Batch 800/938] [D loss: 0.2511] [G loss: 2.9661]\n",
            "[Epoch 26/100] [Batch 0/938] [D loss: 0.2504] [G loss: 3.9710]\n",
            "[Epoch 26/100] [Batch 200/938] [D loss: 0.2158] [G loss: 3.5072]\n",
            "[Epoch 26/100] [Batch 400/938] [D loss: 0.2115] [G loss: 4.4647]\n",
            "[Epoch 26/100] [Batch 600/938] [D loss: 0.2420] [G loss: 2.5326]\n",
            "[Epoch 26/100] [Batch 800/938] [D loss: 0.2819] [G loss: 3.0206]\n",
            "[Epoch 27/100] [Batch 0/938] [D loss: 0.2540] [G loss: 2.7422]\n",
            "[Epoch 27/100] [Batch 200/938] [D loss: 0.2642] [G loss: 3.6020]\n",
            "[Epoch 27/100] [Batch 400/938] [D loss: 0.2168] [G loss: 3.4576]\n",
            "[Epoch 27/100] [Batch 600/938] [D loss: 0.1576] [G loss: 3.5356]\n",
            "[Epoch 27/100] [Batch 800/938] [D loss: 0.2921] [G loss: 3.5771]\n",
            "[Epoch 28/100] [Batch 0/938] [D loss: 0.2946] [G loss: 2.5907]\n",
            "[Epoch 28/100] [Batch 200/938] [D loss: 0.3355] [G loss: 3.2378]\n",
            "[Epoch 28/100] [Batch 400/938] [D loss: 0.1452] [G loss: 3.5727]\n",
            "[Epoch 28/100] [Batch 600/938] [D loss: 0.2945] [G loss: 2.8694]\n",
            "[Epoch 28/100] [Batch 800/938] [D loss: 0.4158] [G loss: 2.4851]\n",
            "[Epoch 29/100] [Batch 0/938] [D loss: 0.3238] [G loss: 2.9882]\n",
            "[Epoch 29/100] [Batch 200/938] [D loss: 0.2626] [G loss: 4.4621]\n",
            "[Epoch 29/100] [Batch 400/938] [D loss: 0.3381] [G loss: 2.6374]\n",
            "[Epoch 29/100] [Batch 600/938] [D loss: 0.1578] [G loss: 3.2972]\n",
            "[Epoch 29/100] [Batch 800/938] [D loss: 0.2810] [G loss: 2.8786]\n",
            "[Epoch 30/100] [Batch 0/938] [D loss: 0.3395] [G loss: 2.1497]\n",
            "[Epoch 30/100] [Batch 200/938] [D loss: 0.2460] [G loss: 2.7168]\n",
            "[Epoch 30/100] [Batch 400/938] [D loss: 0.2456] [G loss: 2.9690]\n",
            "[Epoch 30/100] [Batch 600/938] [D loss: 0.3065] [G loss: 3.5267]\n",
            "[Epoch 30/100] [Batch 800/938] [D loss: 0.3521] [G loss: 2.5225]\n",
            "[Epoch 31/100] [Batch 0/938] [D loss: 0.2108] [G loss: 2.3532]\n",
            "[Epoch 31/100] [Batch 200/938] [D loss: 0.3123] [G loss: 3.0438]\n",
            "[Epoch 31/100] [Batch 400/938] [D loss: 0.2538] [G loss: 3.2317]\n",
            "[Epoch 31/100] [Batch 600/938] [D loss: 0.2579] [G loss: 3.2968]\n",
            "[Epoch 31/100] [Batch 800/938] [D loss: 0.3771] [G loss: 1.6980]\n",
            "[Epoch 32/100] [Batch 0/938] [D loss: 0.3602] [G loss: 3.2880]\n",
            "[Epoch 32/100] [Batch 200/938] [D loss: 0.2645] [G loss: 2.1209]\n",
            "[Epoch 32/100] [Batch 400/938] [D loss: 0.3296] [G loss: 2.4455]\n",
            "[Epoch 32/100] [Batch 600/938] [D loss: 0.3277] [G loss: 2.2432]\n",
            "[Epoch 32/100] [Batch 800/938] [D loss: 0.2827] [G loss: 2.2743]\n",
            "[Epoch 33/100] [Batch 0/938] [D loss: 0.3467] [G loss: 3.0103]\n",
            "[Epoch 33/100] [Batch 200/938] [D loss: 0.2859] [G loss: 2.8287]\n",
            "[Epoch 33/100] [Batch 400/938] [D loss: 0.3425] [G loss: 2.3137]\n",
            "[Epoch 33/100] [Batch 600/938] [D loss: 0.2565] [G loss: 2.8746]\n",
            "[Epoch 33/100] [Batch 800/938] [D loss: 0.2236] [G loss: 2.5878]\n",
            "[Epoch 34/100] [Batch 0/938] [D loss: 0.3237] [G loss: 2.2281]\n",
            "[Epoch 34/100] [Batch 200/938] [D loss: 0.3049] [G loss: 2.0425]\n",
            "[Epoch 34/100] [Batch 400/938] [D loss: 0.2377] [G loss: 2.2871]\n",
            "[Epoch 34/100] [Batch 600/938] [D loss: 0.4111] [G loss: 2.6295]\n",
            "[Epoch 34/100] [Batch 800/938] [D loss: 0.3781] [G loss: 2.5878]\n",
            "[Epoch 35/100] [Batch 0/938] [D loss: 0.2908] [G loss: 2.1865]\n",
            "[Epoch 35/100] [Batch 200/938] [D loss: 0.2636] [G loss: 2.0919]\n",
            "[Epoch 35/100] [Batch 400/938] [D loss: 0.3195] [G loss: 2.0908]\n",
            "[Epoch 35/100] [Batch 600/938] [D loss: 0.4899] [G loss: 1.2194]\n",
            "[Epoch 35/100] [Batch 800/938] [D loss: 0.3823] [G loss: 2.3622]\n",
            "[Epoch 36/100] [Batch 0/938] [D loss: 0.3370] [G loss: 2.1953]\n",
            "[Epoch 36/100] [Batch 200/938] [D loss: 0.3140] [G loss: 1.6490]\n",
            "[Epoch 36/100] [Batch 400/938] [D loss: 0.3374] [G loss: 2.2862]\n",
            "[Epoch 36/100] [Batch 600/938] [D loss: 0.3283] [G loss: 2.2449]\n",
            "[Epoch 36/100] [Batch 800/938] [D loss: 0.3561] [G loss: 1.8438]\n",
            "[Epoch 37/100] [Batch 0/938] [D loss: 0.3418] [G loss: 2.5758]\n",
            "[Epoch 37/100] [Batch 200/938] [D loss: 0.4248] [G loss: 2.6668]\n",
            "[Epoch 37/100] [Batch 400/938] [D loss: 0.3750] [G loss: 2.4673]\n",
            "[Epoch 37/100] [Batch 600/938] [D loss: 0.4366] [G loss: 2.2623]\n",
            "[Epoch 37/100] [Batch 800/938] [D loss: 0.4167] [G loss: 2.2612]\n",
            "[Epoch 38/100] [Batch 0/938] [D loss: 0.4398] [G loss: 2.0389]\n",
            "[Epoch 38/100] [Batch 200/938] [D loss: 0.4005] [G loss: 1.7636]\n",
            "[Epoch 38/100] [Batch 400/938] [D loss: 0.3068] [G loss: 2.8990]\n",
            "[Epoch 38/100] [Batch 600/938] [D loss: 0.3641] [G loss: 2.0597]\n",
            "[Epoch 38/100] [Batch 800/938] [D loss: 0.2956] [G loss: 1.9660]\n",
            "[Epoch 39/100] [Batch 0/938] [D loss: 0.4351] [G loss: 3.0163]\n",
            "[Epoch 39/100] [Batch 200/938] [D loss: 0.4372] [G loss: 1.3552]\n",
            "[Epoch 39/100] [Batch 400/938] [D loss: 0.4463] [G loss: 2.2365]\n",
            "[Epoch 39/100] [Batch 600/938] [D loss: 0.5281] [G loss: 1.9792]\n",
            "[Epoch 39/100] [Batch 800/938] [D loss: 0.4265] [G loss: 1.6442]\n",
            "[Epoch 40/100] [Batch 0/938] [D loss: 0.4299] [G loss: 1.4444]\n",
            "[Epoch 40/100] [Batch 200/938] [D loss: 0.5419] [G loss: 1.7671]\n",
            "[Epoch 40/100] [Batch 400/938] [D loss: 0.4109] [G loss: 1.7207]\n",
            "[Epoch 40/100] [Batch 600/938] [D loss: 0.3687] [G loss: 1.9224]\n",
            "[Epoch 40/100] [Batch 800/938] [D loss: 0.4488] [G loss: 2.2045]\n",
            "[Epoch 41/100] [Batch 0/938] [D loss: 0.3391] [G loss: 2.1278]\n",
            "[Epoch 41/100] [Batch 200/938] [D loss: 0.3839] [G loss: 1.7040]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "img_shape = (1, 28, 28)\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "epochs = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create directory to save samples\n",
        "os.makedirs(\"gan_samples\", exist_ok=True)\n",
        "\n",
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Configure data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Sample visualization function\n",
        "def sample_images(epoch, n_row=4):\n",
        "    \"\"\"Saves a grid of generated digits\"\"\"\n",
        "    z = torch.randn(n_row**2, latent_dim, device=device)\n",
        "    gen_imgs = generator(z)\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale from [-1, 1] to [0, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(gen_imgs.cpu().detach(), nrow=n_row).permute(1, 2, 0))\n",
        "    fig.savefig(f\"gan_samples/epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Training loop\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "        # Adversarial ground truths\n",
        "        valid = torch.ones(imgs.size(0), 1, device=device)\n",
        "        fake = torch.zeros(imgs.size(0), 1, device=device)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = imgs.to(device)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss for real images\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "\n",
        "        # Generate fake images\n",
        "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate fake images\n",
        "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Generator wants discriminator to think these are real\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Save losses\n",
        "        G_losses.append(g_loss.item())\n",
        "        D_losses.append(d_loss.item())\n",
        "\n",
        "        # Print progress\n",
        "        if i % 200 == 0:\n",
        "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
        "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "\n",
        "    # Save sample images at specific epochs\n",
        "    if epoch == 0 or epoch == 50 or epoch == 99:\n",
        "        sample_images(epoch)\n",
        "\n",
        "# Plot training losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses, label=\"Generator\")\n",
        "plt.plot(D_losses, label=\"Discriminator\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"gan_samples/loss_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}